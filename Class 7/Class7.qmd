---
title: "Class 7"
author: "Kavi (PID: A69046927)"
toc: true
format: pdf
---

Today, we will begin our exploration of some "classical" machine learning approaches. We will start with clustering:

Let's first male∈up some data cluster where we know what the answer should be:

```{r}
hist(rnorm(1000))
```
```{r}
x <- c(rnorm(30,mean=-3),rnorm(30,mean=3))
y <- rev(x)

x <- cbind(x,y)
```
 
 A wee peak at x with `plot()`
 
```{r}
plot(x)
```
The main function in "base" R for K-means clustering is called `kmeans()`.

```{r}
k <- kmeans(x,centers=4)
k
```

> Q. How big are the clusters (i.e. their size)?

```{r}
k$size
```

> Q. What clusters do my data points reside in?

```{r}
k$cluster
```

> Q. Make a plot of our data colored by cluster alignment - i.e. make a result figure...

```{r}
plot(x, col=k$cluster)
points(k$centers,col="blue",pch=15)
```

> Q. Run kmeans with center (i.e. values of k) equal 1 to 6

```{r}
k1 <- kmeans(x,centers=1)$tot.withiness
k2 <- kmeans(x,centers=2)$tot.withiness
k3 <- kmeans(x,centers=3)$tot.withiness
k4 <- kmeans(x,centers=4)$tot.withiness
k5 <- kmeans(x,centers=5)$tot.withiness
k6 <- kmeans(x,centers=6)$tot.withiness

ans <- c(k1,k2,k3,k4,k5,k6)
```

Or use a for loop:
```{r}
ans <- NULL
for(i in 1:6) {
  ans <- c(ans,kmeans(x,centers=i)$tot.withinss)
}
ans
plot(ans,typ="b")
```

## Hierarchical Clustering

The main function in "base" R for this is called `hclust()`.


```{r}
d <- dist(x)
hc <- hclust(d)
hc
```

```{r}
plot(hc)
abline(h=7,col="red")
```

To obtain clusters from our `hclust` output result object **hc** we "cut" the tree to yield different sub branches. For this, we use the `cutree()` function.

```{r}
grps <- cutree(hc,h=7)
grps
```
```{r}
plot(x,col=grps)
```
```{r}
library(pheatmap)

pheatmap(x)
```

## Principal Component Analysis (PCA)

```{r}
UKfoods <- read.csv("https://tinyurl.com/UK-foods")
```

Q1. How many rows and columns are in your new data frame named x? What R functions could you use to answer this questions?
```{r}
dim(UKfoods)
```
```{r}
head(UKfoods)
```
```{r}
# Note how the minus indexing works
rownames(UKfoods) <- UKfoods[,1]
UKfoods <- UKfoods[,-1]
head(UKfoods)
```
```{r}
UKfoods <- read.csv("https://tinyurl.com/UK-foods", row.names=1)
head(UKfoods)
```
Q2. Which approach to solving the ‘row-names problem’ mentioned above do you prefer and why? Is one approach more robust than another under certain circumstances?

The second one

```{r}
barplot(as.matrix(UKfoods), beside=T, col=rainbow(nrow(UKfoods)))
```
Q3: Changing what optional argument in the above barplot() function results in the following plot?
```{r}
barplot(as.matrix(UKfoods), beside=F, col=rainbow(nrow(UKfoods)))
```
```{r}
library(ggplot2)
library(tidyr)
library(tibble)
UKfoods_long <- UKfoods |> 
          tibble::rownames_to_column("Food") |> 
          pivot_longer(cols = -Food, 
                       names_to = "Country", 
                       values_to = "Consumption")
head(UKfoods_long)
dim(UKfoods_long)
ggplot(UKfoods_long) +
  aes(x = Country, y = Consumption, fill = Food) +
  geom_col(position = "dodge") +
  theme_bw()
```
Q4: Changing what optional argument in the above ggplot() code results in a stacked barplot figure?
```{r}
head(UKfoods_long)
dim(UKfoods_long)
ggplot(UKfoods_long) +
  aes(x = Country, y = Consumption, fill = Food) +
  geom_col(position = "stack") +
  theme_bw()
```


Q5: Generating all pairwise plots may help somewhat. Can you make sense of the following code and resulting figure? What does it mean if a given point lies on the diagonal for a given plot?
```{r}
pairs(UKfoods, col=rainbow(10), pch=16)
```
```{r}
library(pheatmap)
pheatmap(UKfoods)
```
Q6. What is the main differences between N. Ireland and the other countries of the UK in terms of this data-set?

North Ireland eats more potatoes and less other meat than other countries.

## PCA to the rescue
The main function in "base" R for PCA is called `prcomp()`.

As we want to do PCA on the food data for the different countries we will want the foods in the columns.

```{r}
# Use the prcomp() PCA function
pca <- prcomp( t(UKfoods) )
summary(pca)
```
Our result object is called `pca` and it has a `$x` component that we will look at first.

```{r}
pca$x
```
Q7. Complete the code below to generate a plot of PC1 vs PC2. The second line adds text labels over the data points.
Q8. Customize your plot so that the colors of the country names match the colors in our UK and Ireland map and table at start of this document.

```{r}
library(ggplot2)

cols <- c("orange", "red", "blue", "darkgreen")
ggplot(pca$x) +
  aes(x = PC1, y = PC2, label = rownames(pca$x)) +
  geom_point(color = "darkgreen") +
  geom_text(color=cols)
```
Another major result out of PCA is the so-called "variable loadings" or `$rotation` that tells us how the original variables (foods) contribute to PCs (the new axis)

```{r}
pca$rotation
```
```{r}
ggplot(pca$rotation) +
  aes(PC1,reorder(rownames(pca$rotation),PC1)) +
  geom_col() +
  xlab("PC1 Loading Score")
```

Q7. Complete the code below to generate a plot of PC1 vs PC2. The second line adds text labels over the data points.

# Plot PC1 vs PC2
```{r}
plot(pca$x[,1], pca$x[,2], xlab = "PC1", ylab = "PC2", xlim = c(-270, 500))
text(pca$x[,1], pca$x[,2], colnames(UKfoods))
```